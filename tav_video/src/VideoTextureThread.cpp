/*
 *  VideoTextureThread.cpp
 *  Timeart Visualizer modern OpenGL framework
 *
 *  Created by Sven Hahne on 29.05.11.
 *  Copyright 2011 Sven Hahne. All rights reserved.
 *
 *  problem: wenn threads vom opengl thread gestartet werden passiert murks,
 *  deshalb thread am anfang starten und per flag aktivieren oder nur warten lassen
 *
 * !!!!!!!!!!! Im setup muss bei diesem Scenenode ein vtex0 angegeben werden !!!!!!!!!!!!!!!!
 *   */

#include "VideoTextureThread.h"



namespace tav {

VideoTextureThread::VideoTextureThread(char * file, int initFrameRate)
: cur_frame(0), textureSize(512), dst_pix_fmt(AV_PIX_FMT_BGRA),
  nrBufferFrames(16), actBuffer(0), run(true), m_pause(true), last_frame(-1)
{

	std::cout << " creating new VideoTextureThread " << std::endl;


	texIDs = new GLuint[nrBufferFrames];
	textures = new TextureManager[nrBufferFrames];

	// Register all formats and codecs
	av_register_all();
	av_log_set_level(AV_LOG_INFO);
	av_init_packet(&packet);

	pFormatCtx = avformat_alloc_context();
	pFrame = av_frame_alloc(); // Allocate video frame
	if (!pFrame) fprintf(stderr, "Could not allocate video frame\n");

	// Open video file
	if( avformat_open_input( &pFormatCtx, file, NULL, NULL ) != 0 )
		std::cerr << "Error while calling avformat_open_input (probably invalid file format)" << std::endl;

	// Retrieve stream information
	if( avformat_find_stream_info(pFormatCtx, NULL) < 0 )
		std::cerr << "Error while calling avformat_find_stream_info" << std::endl;

	// Dump information about file onto standard error
	av_dump_format(pFormatCtx, 0, file, false);

	// Find the first video stream
	videoStream=-1;
	for(i=0; i<pFormatCtx->nb_streams; i++)
	{
		if(pFormatCtx->streams[i]->codecpar->codec_type == AVMEDIA_TYPE_VIDEO)
		{
			videoStream = i;
			break;
		}
	}

	if(videoStream == -1) std::cout << "Didn't find a video stream" << std::endl;

	pCodec = avcodec_find_decoder_by_name("h264_vaapi");

	// Get a pointer to the codec context for the video stream
	//pCodec = avcodec_find_decoder(pFormatCtx->streams[videoStream]->codecpar->codec_id);
	pCodecCtx = avcodec_alloc_context3(pCodec);
	if (!pCodecCtx) std::cerr << "Could not allocate audio codec context" << std::endl;

	// open it
	if (avcodec_open2(pCodecCtx, pCodec, NULL) < 0)
		std::cerr << "Could not open codec" << std::endl;

	// Hack to correct wrong frame rates that seem to be generated by some codecs
	if(pCodecCtx->time_base.num>1000 && pCodecCtx->time_base.den==1)
		pCodecCtx->time_base.den=1000;

	frameRate = float(pFormatCtx->streams[videoStream]->avg_frame_rate.num)
					/ float(pFormatCtx->streams[videoStream]->avg_frame_rate.den);


	// Determine total nr of frames
	stream_total_frames = pFormatCtx->streams[videoStream]->duration;
	if (stream_total_frames == 0)
		stream_total_frames = pFormatCtx->duration/1000000.0 * frameRate;

	if(stream_total_frames==0)
	{
		std::cout << "Codec not determine nr frames" << std::endl;
	} else {
		std::cout << "total frames: " << stream_total_frames << std::endl;
	}

	stream_index = pFormatCtx->streams[videoStream]->index;
	stream_width = pFormatCtx->streams[videoStream]->codecpar->width;
	stream_height = pFormatCtx->streams[videoStream]->codecpar->height;
	std::cout << "stream_width: " << stream_width << " stream_height: " << stream_height << std::endl;

	stream_nr = videoStream;

	// Allocate an AVFrame structure
	pFrameRGB = new AVFrame*[nrBufferFrames];
	for (int i=0; i<nrBufferFrames; i++)
	{
		pFrameRGB[i] = av_frame_alloc();
		if(pFrameRGB[i] == NULL) std::cout << "Codec not allocate frame" << std::endl;
	}

	// Determine required buffer size and allocate buffer
	numBytes = av_image_get_buffer_size(dst_pix_fmt, stream_width, stream_height, 1);
	//std::cout << "numBytes per frame: " << numBytes << std::endl;

	frame_num_bytes = numBytes;
	buffer = new uint8_t*[nrBufferFrames];

	for (int i=0; i<nrBufferFrames; i++)
	{
		buffer[i] = new uint8_t[numBytes];

		// Assign appropriate parts of buffer to image planes in pFrameRGB
		//		avpicture_fill((AVPicture *)pFrameRGB[i], buffer[i], dst_pix_fmt,
		//				pCodecCtx->width, pCodecCtx->height); // deprecated
		av_image_fill_arrays(pFrameRGB[i]->data, pFrameRGB[i]->linesize, buffer[i],
				dst_pix_fmt, stream_width, stream_height, 1);
	}

	img_convert_ctx = sws_getContext(
			stream_width, stream_height,
			(AVPixelFormat) pFormatCtx->streams[videoStream]->codecpar->format,
			stream_width, stream_height,
			dst_pix_fmt,
			SWS_FAST_BILINEAR, //SWS_BICUBIC,
			NULL, NULL, NULL);

	if (img_convert_ctx == NULL)
		std::cout << "Cannot initialize the conversion context" << std::endl;


	for (int i=0;i<nrBufferFrames;i++)
	{
		texIDs[i] = textures[i].allocate(stream_width, stream_height, GL_RGBA8, GL_RGBA, GL_TEXTURE_2D);
		textures[i].setWraping(GL_CLAMP_TO_BORDER);

		// pre buffering
		double time = 1.f / static_cast<float>(initFrameRate) * static_cast<float>(i);
		get_frame_pointer(time, true);
		nrFramesBuffered++;
	}


//	thread = new std::thread(&VideoTextureThread::processQueue, this);
}

//------------------------------------------------------------------------------------

VideoTextureThread::~VideoTextureThread()
{
	// Free the RGB image
	for (int i=0; i<nrBufferFrames; i++)
		delete [] buffer[i];

	delete [] buffer;

	av_free(pFrameRGB);

	// Free the YUV frame
	av_free(pFrame);

	// Close the codec
	avcodec_close(pCodecCtx);

	// Close the video file
	avformat_close_input(&pFormatCtx);

	// Free the packet that was allocated by av_read_frame
	av_packet_unref(&packet);
}

//------------------------------------------------------------------------------------

void VideoTextureThread::get_frame_pointer(double time, bool loop)
{
	bool gotFrame = false;
	double timeBase = double(pFormatCtx->streams[videoStream]->time_base.num)
			/ double(pFormatCtx->streams[videoStream]->time_base.den);

	//int frame = static_cast<int> (time * pFormatCtx->streams[videoStream]->avg_frame_rate.num
		//	/ pFormatCtx->streams[videoStream]->avg_frame_rate.den);

	int frame = static_cast<int>(time / timeBase);
//	std::cout << "time: " << time << " frame: " << frame << std::endl;
//	std::cout << "duration: " << pFormatCtx->streams[videoStream]->duration << std::endl;

	if (frame < 0) frame = 0;
	frame = frame % pFormatCtx->streams[videoStream]->duration;

    if (frame != last_frame)
	{
    	// seek position
//    	int ret = av_seek_frame(pFormatCtx, stream_index, frame, AVSEEK_FLAG_BACKWARD); // nur keyframes!!!
//    	if (ret < 0)
//    		std::cout << "could not seek to position" << ((double) 10 / AV_TIME_BASE) << std::endl;

//	//	std::cout << " seek to position " << frame << std::endl;
//    	int ret = avformat_seek_file(pFormatCtx, videoStream, frame - 1, frame, frame +1, AVSEEK_FLAG_ANY);
//    	if (ret < 0)
//    		std::cout << "could not seek to position" << frame << std::endl;

		std::cout << "packet.size: " << packet.size << std::endl;

		/*
		// read the next frame of the stream, should be only one frame
		// groesse variiert
    	while(!gotFrame)
    	{
    		int ret = av_read_frame(pFormatCtx, &packet);
    		std::cout << "av_read_frame packet.size: " << packet.size << std::endl;

        	if ( ret >= 0 && packet.stream_index == videoStream )
        	{
        		std::cout << "got video image" << std::endl;
        		pFrame->width = stream_width;
        		pFrame->height = stream_height;
        		std::cout << "pFrame width " << pFrame->width << std::endl;

        		ret = avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished, &packet);
            	std::cout << "avcodec_decode_video2 returned: " << ret << std::endl;
            	std::cout << "frameFinished " << frameFinished << std::endl;
        		std::cout << "pFrame width " << pFrame->width << std::endl;

            	if (ret <= 0) frameFinished = false;


        	    // Did we get a video frame?
        	    if(frameFinished)
        	    {
            		std::cout << "frameFinished converting image to " << actBuffer << std::endl;

            		readBuffer = actBuffer;

        	    	// Convert the image from its native format to RGB
        			sws_scale(img_convert_ctx,
        					pFrame->data, pFrame->linesize, 0, stream_height,
        					pFrameRGB[actBuffer]->data, pFrameRGB[actBuffer]->linesize);

        			actBuffer = (actBuffer + 1) % nrBufferFrames;

            	    gotFrame = true;
        	    }


            	std::cout << "packet red, packet size " << packet.size << std::endl;
            	std::cout << "packet.stream_index " << packet.stream_index << std::endl;

//            	AVPacket* withPadding;
//            	av_init_packet(withPadding);
//            	withPadding->size = packet.size + AV_INPUT_BUFFER_PADDING_SIZE;

            //	av_packet_ref(withPadding, &packet);

//            	packet.data = 0;
//            	packet.size = 0;

                ret = avcodec_send_packet(pCodecCtx, &packet);

        		if (ret == 0){
                	std::cerr << "avcodec_send_packet success" << std::endl;
        		} else
        		{
        			std::cerr << "avcodec_send_packet error: ";

            		if (ret == AVERROR(EAGAIN))
            			std::cerr << "input is not accepted right now. the packet must be resent after trying to read output  " << std::endl;

            		if (ret == AVERROR(EINVAL))
            			std::cerr << " codec not opened, it is an encoder, or requires flush: " << std::endl;

            		if (ret == AVERROR(EOF))
                    	std::cerr << " the decoder has been flushed, and no new packets can be sent to it (also returned if more than 1 flush packet is sent): " << std::endl;

            		if (ret == AVERROR(ENOMEM))
            			std::cerr << " failed to add packet to internal queue, or similar other errors: legitimate decoding errors " << std::endl;
        		}

        	}


        	// free packet
        //	av_packet_unref(&packet);

    	}
*/

    	/*

    	std::cout << "read frame " << frame << std::endl;
    	int ret = av_read_frame(pFormatCtx, &packet);
		if (ret != 0)
			std::cerr << "av_read_frame error: " << ret << std::endl;

		// Is this a packet from the video stream?
    	if (packet.stream_index == stream_nr)
    	{
        	std::cout << "decode,stream_nr:  " << stream_nr << std::endl;
            int got_frame = 0;



                ret = avcodec_send_packet(pCodecCtx, &packet);

                // In particular, we don't expect AVERROR(EAGAIN), because we read all
                // decoded frames with avcodec_receive_frame() until done.
                //if (ret < 0) return ret == AVERROR_EOF ? 0 : ret;

            ret = avcodec_receive_frame(pCodecCtx, decodedFrame);

            if (ret < 0 && ret != AVERROR(EAGAIN) && ret != AVERROR_EOF)
            {}

            if (ret >= 0)
                got_frame = 1;
        	std::cout << "got_frame: " << got_frame << std::endl;


//    		// Decode video frame
//			ret = avcodec_send_packet(pCodecCtx, &packet);
//			//avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished, &packet); // deprecated
//			if (ret != 0)
//				std::cerr << "avcodec_send_packet error: " << ret << std::endl;
//
//			while(!success)
//			{
//				success = avcodec_receive_frame(pCodecCtx, decodedFrame);
//			}

			sws_scale(img_convert_ctx,
					decodedFrame->data, decodedFrame->linesize, 0, stream_height,
					pFrameRGB[actBuffer]->data, pFrameRGB[actBuffer]->linesize);

    	}


    	// actBuffer wird linear hochgezählt
    	actBuffer = (actBuffer + 1) % nrBufferFrames;
	 */

    	last_frame = frame;
	}

	//  printf("zähle actbuf hoch %d \n", actBuffer);
}

//------------------------------------------------------------------------------------

void VideoTextureThread::stopReset()
{
	internTime = 0;
}

//------------------------------------------------------------------------------------

void VideoTextureThread::updateDt(double time, bool startStop)
{
	//	actDt = newDt;
	//	std::cout << "dt: " << newDt << std::endl;

	internTime = time;

	// brute force methoden zum neustarten, nicht dauerhaft...

	// wenn stop aufgerufen, setze die counter zurück und fülle den buffer mit
	// den ersten frames
	/*
	if (!startStop)
	{
		printf(" VideoTextureThread::updateDt startStop=true\n");

		if (!m_pause)
		{
			printf(" VideoTextureThread isRunning\n");

                requestStop = true;

                // halte an
                join();

                // setze die zeit und counter zurück
                internTime = 0.0;
                nrFramesBuffered = 0;
                readBuffer = nrBufferFrames -1;
                waitReadBuf = 0;
                waitReadBuf = nrBufferFrames -1;
                actBuffer = 1;

                // buffere die ersten paar frames
                for (int i=0;i<nrBufferFrames;i++)
                {
                    // pre buffering
                    double time = static_cast<float>(useDt) * static_cast<float>(i);
                   // get_frame_pointer(time, true);
                }

                nrFramesBuffered = nrBufferFrames;
                internTime = static_cast<float>(useDt) * static_cast<float>(nrBufferFrames-1);
		}
	} else
	{
	 */

	//	if(!m_pause)
	//	{
	//		printf(" VideoTextureThread::updateDt \n");
	//		get_frame_pointer(time, true);
	//	}
}

//------------------------------------------------------------------------------------

void VideoTextureThread::set_paused(bool new_value)
{
	{
		std::unique_lock<std::mutex> lock(m_pause_mutex);
		m_pause = new_value;
	}

	m_pause_changed.notify_all();
}

//------------------------------------------------------------------------------------

bool VideoTextureThread::isPausing()
{
	return m_pause;
}

//------------------------------------------------------------------------------------

void VideoTextureThread::processQueue()
{
	while (run)
	{
		std::unique_lock<std::mutex> lock(m_pause_mutex);

		/*
		while(m_pause)
		{
			printf("VideoTextureThread pausing \n");
			m_pause_changed.wait(lock);
		}
		*/

	//	if ( internTime != lastTime )
	//	{
			mutex.lock();
			get_frame_pointer(internTime);
			mutex.unlock();

			std::cout << "lock" << std::endl;
			m_pause_changed.wait(lock);

			lastTime = internTime;
	//	}
	}
}

//------------------------------------------------------------------------------------

void VideoTextureThread::loadFrameToTexture()
{
	mutex.lock();

	//if (!m_pause){

		std::cout << "uploading image " << readBuffer << std::endl;

		glBindTexture(GL_TEXTURE_2D, texIDs[0]);
		glTexSubImage2D(GL_TEXTURE_2D,             // target
				0,                          // First mipmap level
				0, 0,                       // x and y offset
				stream_width,              // width and height
				stream_height,
				GL_BGRA,
				GL_UNSIGNED_BYTE,
				pFrameRGB[readBuffer]->data[0]);

		m_pause_changed.notify_all();


//		readBuffer = (readBuffer + 1) % nrBufferFrames;

		// frame wurde geholt, zähle die anzahl gepufferter frames runter
//		nrFramesBuffered = nrFramesBuffered -1;
//		if (nrBufferFrames < 0) printf("buffer underun!!\n");


	mutex.unlock();
}


}
